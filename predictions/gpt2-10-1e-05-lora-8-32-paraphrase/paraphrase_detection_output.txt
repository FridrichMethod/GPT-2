Loaded 283003 train examples from data/quora-train.csv
Loaded 40429 train examples from data/quora-dev.csv
gpt_layers.0.self_attention.query.lora_A
gpt_layers.0.self_attention.query.lora_B
gpt_layers.0.self_attention.key.lora_A
gpt_layers.0.self_attention.key.lora_B
gpt_layers.0.self_attention.value.lora_A
gpt_layers.0.self_attention.value.lora_B
gpt_layers.1.self_attention.query.lora_A
gpt_layers.1.self_attention.query.lora_B
gpt_layers.1.self_attention.key.lora_A
gpt_layers.1.self_attention.key.lora_B
gpt_layers.1.self_attention.value.lora_A
gpt_layers.1.self_attention.value.lora_B
gpt_layers.2.self_attention.query.lora_A
gpt_layers.2.self_attention.query.lora_B
gpt_layers.2.self_attention.key.lora_A
gpt_layers.2.self_attention.key.lora_B
gpt_layers.2.self_attention.value.lora_A
gpt_layers.2.self_attention.value.lora_B
gpt_layers.3.self_attention.query.lora_A
gpt_layers.3.self_attention.query.lora_B
gpt_layers.3.self_attention.key.lora_A
gpt_layers.3.self_attention.key.lora_B
gpt_layers.3.self_attention.value.lora_A
gpt_layers.3.self_attention.value.lora_B
gpt_layers.4.self_attention.query.lora_A
gpt_layers.4.self_attention.query.lora_B
gpt_layers.4.self_attention.key.lora_A
gpt_layers.4.self_attention.key.lora_B
gpt_layers.4.self_attention.value.lora_A
gpt_layers.4.self_attention.value.lora_B
gpt_layers.5.self_attention.query.lora_A
gpt_layers.5.self_attention.query.lora_B
gpt_layers.5.self_attention.key.lora_A
gpt_layers.5.self_attention.key.lora_B
gpt_layers.5.self_attention.value.lora_A
gpt_layers.5.self_attention.value.lora_B
gpt_layers.6.self_attention.query.lora_A
gpt_layers.6.self_attention.query.lora_B
gpt_layers.6.self_attention.key.lora_A
gpt_layers.6.self_attention.key.lora_B
gpt_layers.6.self_attention.value.lora_A
gpt_layers.6.self_attention.value.lora_B
gpt_layers.7.self_attention.query.lora_A
gpt_layers.7.self_attention.query.lora_B
gpt_layers.7.self_attention.key.lora_A
gpt_layers.7.self_attention.key.lora_B
gpt_layers.7.self_attention.value.lora_A
gpt_layers.7.self_attention.value.lora_B
gpt_layers.8.self_attention.query.lora_A
gpt_layers.8.self_attention.query.lora_B
gpt_layers.8.self_attention.key.lora_A
gpt_layers.8.self_attention.key.lora_B
gpt_layers.8.self_attention.value.lora_A
gpt_layers.8.self_attention.value.lora_B
gpt_layers.9.self_attention.query.lora_A
gpt_layers.9.self_attention.query.lora_B
gpt_layers.9.self_attention.key.lora_A
gpt_layers.9.self_attention.key.lora_B
gpt_layers.9.self_attention.value.lora_A
gpt_layers.9.self_attention.value.lora_B
gpt_layers.10.self_attention.query.lora_A
gpt_layers.10.self_attention.query.lora_B
gpt_layers.10.self_attention.key.lora_A
gpt_layers.10.self_attention.key.lora_B
gpt_layers.10.self_attention.value.lora_A
gpt_layers.10.self_attention.value.lora_B
gpt_layers.11.self_attention.query.lora_A
gpt_layers.11.self_attention.query.lora_B
gpt_layers.11.self_attention.key.lora_A
gpt_layers.11.self_attention.key.lora_B
gpt_layers.11.self_attention.value.lora_A
gpt_layers.11.self_attention.value.lora_B
save the model to gpt2-10-1e-05-paraphrase.pt
Epoch 0: train loss :: 1.021, dev acc :: 0.704
save the model to gpt2-10-1e-05-paraphrase.pt
Epoch 1: train loss :: 0.545, dev acc :: 0.756
save the model to gpt2-10-1e-05-paraphrase.pt
Epoch 2: train loss :: 0.479, dev acc :: 0.783
save the model to gpt2-10-1e-05-paraphrase.pt
Epoch 3: train loss :: 0.450, dev acc :: 0.800
save the model to gpt2-10-1e-05-paraphrase.pt
Epoch 4: train loss :: 0.431, dev acc :: 0.807
Epoch 5: train loss :: 0.418, dev acc :: 0.806
save the model to gpt2-10-1e-05-paraphrase.pt
Epoch 6: train loss :: 0.406, dev acc :: 0.820
save the model to gpt2-10-1e-05-paraphrase.pt
Epoch 7: train loss :: 0.399, dev acc :: 0.823
Epoch 8: train loss :: 0.392, dev acc :: 0.823
save the model to gpt2-10-1e-05-paraphrase.pt
Epoch 9: train loss :: 0.385, dev acc :: 0.828
Loaded model to test from gpt2-10-1e-05-paraphrase.pt
Loaded 40429 train examples from data/quora-dev.csv
Loaded 80858 test examples from data/quora-test-student.csv
dev paraphrase acc :: 0.828
Time: 6:25/epoch
